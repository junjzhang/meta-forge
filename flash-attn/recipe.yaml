context:
  name: "flash-attn"
  git_url: https://github.com/Dao-AILab/flash-attention.git
  git_tag: ${{ git.latest_tag( git_url ) }}

package:
  name: ${{ name }}
  version: ${{ git_tag[1:] }}

source:
  - git: ${{ git_url }}
    tag: ${{ git_tag }}

build:
  number: ${{ build|int + (microarch_level|int) * 100 + 1 }}
  string: py${{ python | version_to_buildstring }}_cuda${{ cuda | version_to_buildstring }}_pt${{ pytorch | replace('.', '') }}
  script:
    - export CUDA_HOME=$PREFIX
    # enable sccache
    # - mkdir -p $SRC_DIR/bin
    # - |
    #   cat << 'EOF' > $SRC_DIR/bin/nvcc
    #   #!/bin/sh
    #   sccache $PREFIX/bin/nvcc "$@"
    #   EOF
    # - chmod +x $SRC_DIR/bin/nvcc
    # - export SCCACHE_LOG=trace
    # - export CUDA_HOME=$SRC_DIR

    - $CUDA_HOME/bin/nvcc -V
    - export MAX_JOBS=$(expr $CPU_COUNT - 16)
    # - export TORCH_CUDA_ARCH_LIST=8.0+PTX
    - export TORCH_CUDA_ARCH_LIST="8.0+PTX 9.0+PTX"
    - export FLASH_ATTENTION_FORCE_BUILD="TRUE"
    - export FLASH_ATTENTION_FORCE_CXX11_ABI="FALSE"
    # - export C="sccache $C"
    # - export CXX="sccache $CXX"
    - pip install . -vvv --no-deps --no-build-isolation
    - sccache --show-stats
    # - $PYTHON setup.py install

requirements:
  host:
    - ${{ compiler('c') }}
    - ${{ compiler('cxx') }}
    # - ${{ compiler('cuda') }}
    - ${{ stdlib('c') }}
    - sccache
    # - ninja
    # host:
    - cuda
    # - cuda_nvcc = ${{ short_cuda }}
    # - cuda_nvcc_${{ target_platform }}
    - cuda-cudart-dev
    - libcusparse-dev
    # - pytorch::pytorch-cuda = ${{ cuda }}
    # - pytorch::pytorch == ${{ pytorch }}
    - pytorch-cuda ${{ cuda }}
    - pytorch
    # - pytorch::torchaudio
    # - pytorch::torchvision
    - python
    - pip

    - psutil
    - python-ninja
    - packaging
    - setuptools
    - lit
    # flash_attn depends
    - einops
    - fsspec
  run:
    - einops
    # - cuda
    - pytorch
    - fsspec
tests:
  - python:
      imports:
        - torch
        - flash_attn
        - flash_attn_2_cuda
